configfile: 'config.yaml'

rule prepare_data:
    ''' Prepare data for training'''
    input:
        script = 'prepare_data.py',
        qcd = config['background'],
    output:
        config['dataset']
    params:
        leptoquarks = config['leptoquarks'],
        ato4l = config['ato4l'],
        hChToTauNu = config['hChToTauNu'],
        hToTauTau = config['hToTauTau'],
        monojet = config['monojet']
    shell:
        'python {input.script} --output-file {output} \
                               --input-file {input.qcd} \
                               --events {wildcards.events} \
                               --input-bsm {params.leptoquarks} \
                               --input-bsm {params.ato4l} \
                               --input-bsm {params.hChToTauNu} \
                               --input-bsm {params.hToTauTau} \
                               --input-bsm {params.monojet}'

rule tune_teacher_kd_ae_l1:
    input:
        data = expand(config['dataset'], events=-1)
    output:
        'output/big_teacher.h5'
    shell:
        'python tune_teacher_hyperparameters.py {input.data} {output}'

rule run_tensorboard:
    params:
        port = 8896
    shell:
        'tensorboard --logdir=output/tb_logs  --port={params.port}'

rule check_big_teacher_performance:
    input:
        dataset = expand(config['dataset'], events=-1),
        model_h5 = rules.tune_teacher_kd_ae_l1.output
    params:
        leptoquarks = config['leptoquarks'],
        ato4l = config['ato4l'],
        hChToTauNu = config['hChToTauNu'],
        hToTauTau = config['hToTauTau'],
        monojet = config['monojet']
    shell:
        'python check_ae_performance.py {input.dataset} \
            --pretrained-ae-h5 {input.model_h5} \
            --signal leptoquark {params.leptoquarks} \
            --signal ato4l {params.ato4l} \
            --signal hChToTauNu {params.hChToTauNu} \
            --signal hToTauTau {params.hToTauTau} \
            --signal monojet {params.monojet} '

rule reformat_ae_l1_data:
    input:
        data = '/content/drive/MyDrive/data_-1.pickle',
        teacher_h5 = rules.tune_teacher_kd_ae_l1.output
    output:
        train_loss = 'output/l1_ae_train_loss.h5',
        test_loss = 'output/l1_ae_test_loss.h5',
        val_loss = 'output/l1_ae_val_loss.h5',
        signal_loss = 'output/l1_ae_signal_loss.h5'
    params:
        log_loss = True
    shell:
        'python reformat_ae_l1_data.py --data-file {input.data} \
                                       --teacher-input-h5 {input.teacher_h5} \
                                       --output-train-loss {output.train_loss} \
                                       --output-test-loss {output.test_loss} \
                                       --output-val-loss {output.val_loss} \
                                       --output-signal-loss {output.signal_loss}\
                                       --log-loss {params.log_loss}'

rule apply_selective_sampling:
    input:
        data_bg = rules.reformat_ae_l1_data.output.train_loss,
        data_sig = rules.reformat_ae_l1_data.output.signal_loss
    params:
        signal_name = '"A to 4 leptons"',
        signal_fraction = 0.25,
        fit_threshold = 1.2,
        plots = 'plots/selective_subsampling/'
    output:
        train_loss_bg = 'output/l1_ae_train_loss_selective.h5',
        train_loss_bg_sig = 'output/l1_ae_train_loss_selective_bg_plus_sig.h5',
        discarded_test_loss = 'output/l1_ae_train_loss_discarded.h5',
        signal_loss = 'output/l1_ae_signal_loss_selective.h5'
    shell:
        'mkdir -p {params.plots};'
        'python selective_sampling.py --data-file-bg {input.data_bg} \
                                       --data-file-sig {input.data_sig} \
                                       --signal-name {params.signal_name} \
                                       --signal-fraction {params.signal_fraction} \
                                       --fit-threshold {params.fit_threshold} \
                                       --outfile-train-loss-bg {output.train_loss_bg} \
                                       --outfile-train-loss-bg-sig {output.train_loss_bg_sig} \
                                       --outfile-discarded-test-loss {output.discarded_test_loss} \
                                       --outfile-signal-loss {output.signal_loss} \
                                       --plot-dir {params.plots}'

rule kd_ae_l1_train:
    input:
        train = rules.reformat_ae_l1_data.output.train_loss,
        test = rules.reformat_ae_l1_data.output.test_loss,
        val = rules.reformat_ae_l1_data.output.val_loss,
        signal = rules.reformat_ae_l1_data.output.signal_loss
    params:
        data_name = 'data',
        loss_name = 'teacher_loss',
        distillation_loss = 'mae',
        node_size = 32,
        n_features = 3,
        batch_size = 1024,
        n_epochs = 100,
        plots = 'plots/ae_l1/',
        shuffle_strategy = 'none',
        shuffle_during = 'never'
    output:
        h5 = 'output/student_model-q{quant_size}.h5',
        json = 'output/student_model-q{quant_size}.json',
        result = 'output/student_result-q{quant_size}.h5'
    shell:
        'mkdir -p {params.plots};'
        'python knowledge_distillation.py --input-train-file {input.train} \
                                          --input-test-file {input.test} \
                                          --input-val-file {input.val} \
                                          --input-signal-file {input.signal} \
                                          --data-name {params.data_name} \
                                          --n-features {params.n_features} \
                                          --teacher-loss_name {params.loss_name} \
                                          --output-model-h5 {output.h5} \
                                          --output-model-json {output.json} \
                                          --node-size {params.node_size} \
                                          --batch-size {params.batch_size} \
                                          --n-epochs {params.n_epochs} \
                                          --distillation-loss {params.distillation_loss} \
                                          --quant-size {wildcards.quant_size} \
                                          --output-result {output.result} \
                                          --output-dir {params.plots} \
                                          --particles-shuffle-strategy {params.shuffle_strategy} \
                                          --particles-shuffle-during  {params.shuffle_during} '

rule kd_ae_l1_plot:
    input:
        student = expand(rules.kd_ae_l1_train.output.result, quant_size='{quant_size}'),
        teacher = rules.reformat_ae_l1_data.output.test_loss,
        signal = rules.reformat_ae_l1_data.output.signal_loss
    params:
        loss_name = 'teacher_loss',
    output:
        plots = directory('plots-ae-l1-q{quant_size}/')
    shell:
        'mkdir -p {output.plots};'
        'python plot_results.py --student {input.student} \
                                --teacher {input.teacher} \
                                --teacher-loss-name {params.loss_name} \
                                --output-dir {output.plots} \
                                --signal {input.signal}'

rule quantized_kd_ae_l1_plot:
    ''' Specify to which bit to quantize the student
        use 0 for full precision
    '''
    input:
        expand(rules.kd_ae_l1_plot.output, quant_size=0)

rule tune_student_kd_ae_l1:
    input:
        data = rules.reformat_ae_l1_data.output.train_loss
    params:
        distillation_loss = 'mae',
    output:
    shell:
        'python tune_student_hyperparameters.py --input-file {input.data} \
                                        --distillation-loss {params.distillation_loss} '

rule kd_graph_train:
    input:
        train = config['train'],
        test = config['test'],
        signal = config['signal']
    params:
        data_name = 'InputParticlesOriginal',
        loss_name = 'loss_pid', # loss_all_reco_chamfer
        distillation_loss = 'mse',
        n_features = 3,
        plots = 'plots/graph/',
        shuffle_strategy = 'shuffle_within_between_pid',
        shuffle_during = 'train_predict'
    output:
        h5 = "output/graph_student_model.h5",
        json = "output/graph_student_model.json",
        result = "output/graph_student_result.h5"
    shell:
        'mkdir -p {params.plots};'
        'python knowledge_distillation.py --input-train-file {input.train} \
                                          --input-test-file {input.test} \
                                          --input-signal-file {input.signal} \
                                          --data-name {params.data_name} \
                                          --n-features {params.n_features} \
                                          --teacher-loss_name {params.loss_name} \
                                          --output-model-h5 {output.h5} \
                                          --output-model-json {output.json} \
                                          --batch-size 256 \
                                          --n-epochs 100 \
                                          --distillation-loss {params.distillation_loss}\
                                          --output-result {output.result} \
                                          --output-dir {params.plots} \
                                          --particles-shuffle-strategy {params.shuffle_strategy} \
                                          --particles-shuffle-during  {params.shuffle_during} '                                          

rule kd_graph_plot:
    input:
        student = rules.kd_graph_train.output.result,
        signal = config['signal'],
        teacher = config['test']
    params:
        loss_name = 'loss_pid',
        plots = "plots/graph/"
    shell:
        'mkdir -p {params.plots};'
        'python plot_results.py --student {input.student} \
                                --teacher {input.teacher} \
                                --teacher-loss-name {params.loss_name} \
                                --output-dir {params.plots} \
                                --signal {input.signal}'


rule co_train:
    input:
        data = config['dataset'],
    params:
        dir = 'cotrain/'
    shell:
        'mkdir -p cotrain;'
        'python co_train.py --data-file {input.data} \
                            --output-dir {params.dir}'


rule co_train_results:
    input:
        data = config['dataset'],
    params:
        train_loss = 'cotrain/l1_ae_train_loss.h5',
        test_loss = 'cotrain/l1_ae_test_loss.h5',
        signal_loss = 'cotrain/l1_ae_signal_loss.h5',
        result = 'cotrain/student_result.h5',
        dir = 'cotrain/'
    shell:
        'mkdir -p cotrain;'
        'python co_train_results.py --data-file {input.data} \
                            --teacher-output-train-loss {params.train_loss} \
                            --teacher-output-test-loss {params.test_loss} \
                            --teacher-output-signal-loss {params.signal_loss}\
                            --student-output-result {params.result}\
                            --output-dir {params.dir}'
                            
rule co_train_plot:
    input:
        student = rules.co_train_results.params.result,
        teacher = rules.co_train_results.params.test_loss,
        signal = rules.co_train_results.params.signal_loss
    params:
        loss_name = 'teacher_loss',
        plots = 'plots/cotrain/'
    shell:
        'mkdir -p {params.plots};'
        'python plot_results.py --student {input.student} \
                                --teacher {input.teacher} \
                                --teacher-loss-name {params.loss_name} \
                                --output-dir {params.plots} \
                                --signal {input.signal}'
                                
rule co_train_latent:
    input:
        data = config['dataset'],
    params:
        dir = 'cotrain_latent/'
    shell:
        'mkdir -p cotrain_latent;'
        'python co_train.py --data-file {input.data} \
                            --output-dir {params.dir}\
                            --include-latent-loss True'
                            
rule co_train_latent_results:
    input:
        data = config['dataset'],
    params:
        train_loss = 'cotrain_latent/l1_ae_train_loss.h5',
        test_loss = 'cotrain_latent/l1_ae_test_loss.h5',
        signal_loss = 'cotrain_latent/l1_ae_signal_loss.h5',
        result = 'cotrain_latent/student_result.h5',
        dir = 'cotrain_latent/'
    shell:
        'mkdir -p cotrain_latent;'
        'python co_train_results.py --data-file {input.data} \
                            --teacher-output-train-loss {params.train_loss} \
                            --teacher-output-test-loss {params.test_loss} \
                            --teacher-output-signal-loss {params.signal_loss}\
                            --student-output-result {params.result}\
                            --output-dir {params.dir}'
                            
rule co_train_latent_plot:
    input:
        student = rules.co_train_latent_results.params.result,
        teacher = rules.co_train_latent_results.params.test_loss,
        signal = rules.co_train_latent_results.params.signal_loss
    params:
        loss_name = 'teacher_loss',
        plots = 'plots/cotrain_latent/'
    shell:
        'mkdir -p {params.plots};'
        'python plot_results.py --student {input.student} \
                                --teacher {input.teacher} \
                                --teacher-loss-name {params.loss_name} \
                                --output-dir {params.plots} \
                                --signal {input.signal}'
